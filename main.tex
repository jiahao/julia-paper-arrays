\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:
% preprint      Remove this option only once the paper is in final form.

% Alan TODO:  Can we actually see some Julia arrays, in their full flexibility

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}

\newcommand{\ALGOL}{A\textsc{LGOL}}
\newcommand{\MATLAB}{\textsc{MATLAB}}
\newcommand{\Mathematica}{\textit{Mathematica}}
\newcommand{\code}[1]{\texttt{#1}}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{ARRAY '14}{June 15, 2014, Edinburgh, UK}
\copyrightyear{2014} 
%\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
%\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{Array Operators Using Multiple Dispatch}    % These are ignored unless
\preprintfooter{Array Operators Using Multiple Dispatch} % 'preprint' option specified.

% \title{Array implementations in Julia}

%\title{Array Operators Using Multiple Dispatch}
%\subt
\title{ Array Operators Using Multiple Dispatch }
\subtitle{A design methodology for array implementations in dynamic languages}

\authorinfo{Jeff Bezanson \and Jiahao Chen \and Stefan Karpinski \and Viral Shah  \and Alan Edelman}
           {MIT Computer Science and Artificial Intelligence Laboratory}
  { bezanson@mit.edu, jiahao@mit.edu, stefan@karpinski.org, viral@mayin.org, edelman@mit.edu }

%TODO Get Dahua to sign on as coauthor
%\authorinfo{Dahua Lin}
%           {Toyota Technological Institute at Chicago}
%           {dhlin@ttic.edu}

\maketitle

\begin{abstract}

Arrays are such a rich and fundamental data type that they tend to be built in to
a language, either in the compiler or in a large low-level library.
% TODO: a sentence about why this is not good and why we need more flexibility
Only a few languages, such as C++ and Haskell, provide the necessary power to define
n-dimensional arrays, but these systems rely on compile-time abstraction to provide
performance, sacrificing some amount of flexibility.
In contrast, dynamic languages make it straightforward for the user to define any
behavior they might want, but at the possible expense of performance.

As part of the Julia language project, we have developed an approach that yields
a novel trade-off between flexibility and compile-time analysis. The core
abstraction we use is multiple dispatch. This has been extensively studied
as a general object-oriented programming technique, but we find it
especially relevant to technical and array-based programming. By expressing
key functions such as array indexing using multi-method
signatures, a surprising range of behaviors can be obtained, in a way that is
both relatively easy to write and amenable to compiler analysis.
The compact factoring of concerns provided by these methods makes it easier
for user-defined types to behave consistently with types in the standard
library.


%pulls out abstractions that might not have been named before
%creates better integration of user-defined types
%flexible enough to change the behavior if you want
%creates more coherence

%Interest in recovering performance in these systems has spurred development of
%analysis frameworks. 

\end{abstract}

%\category{CR-number}{subcategory}{third-level}

\keywords
Julia, multiple dispatch, type inference, array indexing, static analysis,
dynamic dispatch

\section{Introduction}

\begin{quotation}
``Unfortunately, it is very difficult for a designer to select in advance all
the abstractions which the users of his language might need. If a language is
to be used at all, it is likely to be used to solve problems which its
designer did not envision, and for which abstractions embedded in the language
are not sufficient.'' - Ref. \cite{Liskov:1974pb}
\end{quotation}

%BEGIN OUTLINE

%Introduction to multiple dispatch: traditionally it is considered an optimization

%Julia philosophy: Instead of MD as a hack, turn it around and make it a core feature of the language.

%In this paper we show how to use MD to implement semantic features.

A particularly interesting use case is $n$-arrays (arrays of rank $n$, or
simply `array'). While 1-arrays are an essential data type in many programming
languages, multidimensional arrays in their full generality are significantly
more challenging to implement efficiently \cite{Sattley:1960as,
Sattley:1961as, Randell:1964a6}. For example, programs that traverse arrays
require many indexing computations, and for these programs to perform
efficiently, there is a strong incentive for programming languages to
implement static analysis such as constant subexpression elimination and loop
fusion/reordering of array traversals \cite{Busam:1969oe}.

Using arrays an example, we can classify languages into those favoring
static analysis and those favoring full dynamic generality.

%TODO make a nice figure out of this idea
%      | Statically knowable   | Not ~ but general
% -----+-----------------------+-------------------------
% fast | Haskell, C++, Fortran | (Unattainable perfection)
% slow | (Undesirable)         | Matlab, APL, Python/NumPy**
%
%** Everything in this category doesn't really have the flexibility to write general libraries
%   _within the same language_, so for performance you have to use a static language.
%   heroic compilers under the hood are static again.

\subsection{Languages favoring static analysis}

Static languages like Fortran, C/C++ and Haskell fall naturally into this
category. In extreme cases, some static languages only support $n$-arrays up
to some fixed maximal rank, like Fortran.\footnote{The maximal rank was 3 in
FORTRAN I \cite{Backus:1957fa}, increased to 7 in Fortran 77, and later to 15
in Fortran 2008.} The advantage of imposing rank limits is that specialized
implementations for each rank $n$ can be written out in full. The
disadvantage, of course, is that program expressibility is limited and it is
challenging to write libraries for general $n$-arrays in these languages.

Some static languages, however, have more flexibility while
still restricting programs to be amenable to static analysis. A notable
example is C++, where templates allow the implementation of $n$-arrays in performant
libraries, like \code{Blitz++} \cite{Veldhuizen:1998ab} or
\code{Boost.MultiArray} \cite{Garcia:2005ma}.  Nevertheless, template
metaprograms are notoriously difficult to implement and
comprehend. While $n$-arrays can be implemented compactly using recursive
templates \cite{Bavestrelli:2000ct}, peeling off $(n-1)$-arrays
recursively from $n$-arrays, production quality libraries avoid explicit
recurrences, implementing instead a great many special-cased implementations
of $n$-arrays to ensure that C++ compilers unroll loops fully to achieve
maximal performance \cite{Garcia:2005ma}. The recursive peeling design also
restricts indexing semantics in C++ arrays to traverse only from  leftmost to
rightmost index. Furthermore, the behavior of templates is not available at
runtime; C++ programs cannot use arrays
whose ranks are known only at runtime. Also you have handle dimensions one
at a time.

Another notable static language that supports $n$-arrays is Haskell, and in
particular its Repa (Regular Parallel Arrays) library \cite{Keller:2010rs}.
One key abstraction is the \code{Shape} class, which enables dispatch on the
index-to-offset mapping function \cite{Keller:2010rs}. This allows for in-
memory representations of transposed arrays, simply by changing the
\code{Shape} class to redefine the memory striding rules \cite{Keller:2010rs}.
Nevertheless, Repa employs the same recursive peeling semantics of C++ array
libraries: for example, reductions like \code{Sum} are only defined
over the last index \cite{Keller:2010rs}. In principle, a generalized
\code{Sum} over an arbitrary index can be composed as \code{Sum} with pre- and
post-permutations to swap the desired index with the last index. However,
this is semantically more work and furthermore isn't guaranteed to produce a
good memory traversal pattern. Similarly, operations over more than one
dimension are implemented recursively over the dimensions, which means that
the compiler has to make the difficult decision whether or not to unroll the
recursion \cite{Lippmeier:2011ep}. Manual unrolling is of course a possibility
but has the cost of introducing repetition in the code
\cite{Lippmeier:2012gp}.

%Arguably, a purely functional language like Haskell, which disallows
%imperative constructs like loops, forces programmers to identify
%abstractions. With fewer options to implement semantics, semantics is more
%intimately tied to types.

\subsection{Languages favoring dynamic generality}

Conversely, some applications may call for semantics that are not amenable to
static analysis. For example, one can envision applications requiring
arrays whose ranks are known only at runtime. Languages that wish to support
such applications of some applications cannot choose only to use data
structures that fit in a constant amount of memory. Another possibility is
that programs may wish to treat arrays with different numbers of dimensions
very differently. A vector---an intrinsically one-dimensional object---can be
conceptually very different from an $N\times1$ matrix, which is intrinsically
two-dimensional. For example, many linear algebraic computations like the
singular value decomposition make sense only for matrices, not vectors. These
differences in object semantics and behavior make the number of dimensions a
crucial part of program semantics and thus cannot be merely a compiler
implementation detail. Even though it is possible to construct an isomorphism
between column vectors and $N\times1$ matrices, semantic differences preclude
a genuine ``is-a'' relationship, and hence results in a violation of the
Liskov substitution principle \cite{Liskov:1987da}. We can think of this
situation as the linear algebraic counterpart to the canonical circle-ellipse
problem \cite{Halbert:1987ut}. For such applications requiring different
semantics for different ranks at runtime, runtime flexibility is also essential.

These use cases favor dynamic languages which support full runtime
flexibility in arrays, such as \Mathematica{}, \MATLAB{} and Python/NumPy,
and accept that most or all operations will be dynamically dispatched.

However, languages in this category are not without their pitfalls. First,
array semantics may be implemented using flexible data structure that may
cause abstraction leakage. For example, \Mathematica{} implements $n$-arrays
using nested lists of expressions \cite{mathematica:nl, mathematica:int}, with
the \code{[[]]} (\code{Part}) operator playing the role of array indexing.
However, \Mathematica's powerful pattern matching semantics are prone to
abstraction leakage. For example, it is possible to ``index'' an array with
% Jeff says: This is an interesting example of excessive permissiveness,
% but it sounds like a strawman since most dynamic languages have data
% abstraction that works well enough.
more indices than its rank; the extra indices would simply traverse into the
expression contained within the matrix element. For example, \code{Array[f,
{3, 4}][[3, 2]]} returns \code{f[3,2]}, whilst \code{Array[f, {3, 4}][[3, 2,
1]]} returns \code{3}. Thus the semantics of the \code{Part} symbol is more
general than pure array indexing and allows for violations of the array data
abstraction, which may come as a surprise to users. Another example in
\MATLAB{} comes from implementing arrays of user-defined classes. Since
\MATLAB{} arrays are not defined in terms of its object system, special care
must be taken for arrays of user-defined classes to have the same semantic
behavior as arrays of built- in types. In principle, user-defined classes
could otherwise lead to arbitrary behavior.

% Jeff calls this "just overloading".

Second, the flexibility of these languages results in generally slow
performance, and language implementations wishing to attain performance must
instead resort to advanced static analyses anyway. For example, \MATLAB's
array semantics allow an array to be enlarged automatically whenever a write
occurs to an out-of-bounds reference, and also for certain operations to
automatically promote the element type of an array from real to complex
numbers. This poses implementation challenges for static \MATLAB{} compilers
like \code{FALCON}, which have to implement a complete type system with
multiple compiler passes and complex interprocedural flow analyses to check
specifically for such drastic runtime changes to arrays \cite{Rose:1999tt,
Li:2013mf}. Additionally, type checking was essential to disambiguate
\MATLAB{} expressions like \code{A*B}, which, depending on the dimensions of
\code{A} and \code{B}, could represent a scaling, inner product, outer
product, matrix- matrix multiplication, matrix-vector multiplication
\cite{Rose:1999tt}. Similar issues also plague the Octave interpreter over the
implementation of \MATLAB's array semantics \cite{Eaton:2001op}. These
challenges are by no means unique to \MATLAB: the same performance issues has
also led to the development of Hack, a statically typed PHP implementation
which contains a full static type checking system \cite{Verlaguet:2014hn}.
Even despite these heroic efforts, the flexibility of languages in this
category inevitably limits the extent of static analyses (based on a finite-
height partial order over patterns) that is possible over an arbitrary
program.

Third, implementations of such flexible languages often resort to implementing
arrays in libraries in static languages for the sake of performance. This
leads to a disjunction between the implementation of arrays and the native
object system of the language. The machinery of the Python NumPy library is a
good case in point. NumPy arrays are Python objects, but instead of using the
native Python object system's semantic mechanisms, NumPy instead has its own
abstraction mechanisms implemented with a large C code base for compile-time
code generation and custom runtime dispatch mechanisms. The resulting Python
objects are little more than lookup tables of behaviors. In
other languages like R, the disjunction occurs because the array and object
subsystems were added the language at different points in time.

%TODO other languages: R, APL, ZPL, Lisp?

%% To summarize, the flexibility of languages in this category can lead to faulty
%% array abstractions and slow performance. To address performance issues,
%% language implementations must either reimplement large components of compiler
%% infrastructure in order to perform static analyses like type inference, or
%% resort to implementing key components of array infrastructure in external
%% libraries written in a static language like C or Fortran to achieve acceptable
%% performance, with the result that the first language becomes little more than
%% a superficial wrapper.

\subsection{The performance---flexibility trade-off}

The conflicting requirements of performance and flexibility pose a dilemma for
language designers and implementers. Most current languages choose either to
support only programs that are amenable to static analysis for the sake of
performance, such as C++ and Haskell, or choose to support more general
classes of programs, like \MATLAB, \Mathematica, Python/NumPy, and APL. While
dynamic languages nominally give up static analysis in the process, many
implementations of these languages still resort to static analysis in
practice, either by hard-coding array semantics in a compiler, or by
implementing arrays in an external library written in a static language. Thus
we see that having some modicum of static analysis is unavoidable for
reasonable performance.

A key problem with dynamic languages is how to provide more declarative
information to a compiler. Users expect to be able to program any
run-time behavior and have it just work. In theory, a compiler could
partially evaluate programs based on whatever static information it can
find (e.g. integer constants), but it is unlikely that this would reliably
provide performance where it is needed.


%TODO A static # of dimensions is probably sufficient for most applications,
%but there is a large cliff where everything changes if your code is more
%dynamic

\section{Julia Arrays}

Arrays are implemented in Julia\cite{Bezanson:2012jf} as the \code{Array}
data type, which is parameterized by an element type and a rank (an integer).
For purposes of this paper, an \code{Array} can be considered a tuple of
a contiguous memory region and a shape (a tuple of integers giving the size
of the array in each dimension). This simple representation is already
enough to require nontrivial design decisions.

%% Multiple dispatch makes it possible for \code{Array}s to have implementations
%% that afford both generality and efficiency. The most general
%% \code{Array\{Any\}} is an array of pointers to heap-allocated values. However,
%% \code{Array}s of native machine types like double precision floating point
%% numbers (\code{Float64}s) or integers of native machine size (\code{Int}s) can
%% be implemented to be C/Fortran- compatible. Interestingly,
%% \code{Array\{Nothing\}} can be implemented with zero storage space at runtime,
%% since it consists of elements of an immutable type with zero fields, which is
%% possible because only a single instance of any such type can exist, and hence
%% need not actually be stored. This allows for some clever tricks like
%% implementing \code{Set\{T\}} using a \code{Dict\{T,Nothing\}} to avoid paying
%% any cost for the value array in the \code{Dict}.

%TODO identify subthemes in this giant block
The implementation of Julia \code{Array}s differs significantly from other
languages in several important respects. First, automatic type inference is
not used merely as a compiler optimization technique, like in static compilers
for dynamic languages, but instead is a crucial language feature that, in
conjunction with multiple dispatch, are necessary for implement language
semantics. Second, Julia by virtue of being a dynamic language provides the
static analyses enabled by type inference and multiple dispatch at runtime. In
contrast, the C++ expression templates are static semantics that are
unavailable at runtime, and the semantics of templates are for the purposes of
runtime programs effectively a separate language. The single all- runtime
model of Julia is much easier to use and imposes fewer restrictions on the
kind of Julia code that can be written. You can implement a bigger class of
rules in Julia rather than having to special case on the number of dimensions
or be forced to rely on recursive definitions onto subarrays. For example in
Julia you can do \code{A[I...]} where \code{I} is a heterogeneous array, and
all the same definitions are still applicable, but you might take a bit of
performance drop. Third, Julia aggressive employs specialization on argument
types by default, generating a large variety of specialized methods for each
generic function, in contrast with the default of one compilation per function
declaration in most other languages. This is possible thanks to the static
analyses enabled by type inference and multiple dispatch. C++ can do this with
templates, but generating an entire family of related methods must be
requested explicitly using templates. Furthermore, having all the semantics
enabled by static analysis at runtime allows for composition for code that
results in efficient specialized methods by the compiler. For example, Julia
code implementing the algebra of the quaternion field can be composed
transparently with other Julia code implementing Gaussian elimination by the
compiler. One can be inlined into the other and woven into a specialized
method for Gaussian elimination on quaternionic matrices. Contrast this with
the conventional wisdom that user-defined types are slow in languages like
\MATLAB{} and even C++; user-defined classes are reference types rather than
value types, and methods have to be declared \code{virtual} to enable dynamic
dispatch as opposed to the default static dispatch mechanism. In Julia,
everything in principle has to happen dynamically but then the focus is on
analyzing it statically. Fourth, array functions in Julia have access to the
entire argument list, allowing for conceptual models and APIs that are not
restricted to strict left-to-right or right-to-left index traversal that would
be necessary by recursive peeling. Fifth, specialization of methods can be
implemented with multiple dispatch, which is a powerful alternative to the
single dispatch mechanism of Haskell based on function classes and pattern
matching.

In the cases where static analysis can happen, Julia can perform as fast as
languages where array semantics is predicated on static analysis, but multiple
dispatch allows Julia to be more flexible than just this. Julia also allows
for generic fall-back methods that are slower because of a lack of static
analysis, but nonetheless can be dispatched upon. The flexibility that is
allowed by this approach is a very nonobvious application of multiple
dispatch.

\subsection{Array indexing rules}

Whatever decision is made, rules must be defined for how various operators act
on dimensions. For now we will focus on indexing, since selecting parts of
arrays has particularly rich behavior with respect to dimensionality. For
example, if a single row or column of a matrix is selected, does the result
have one or two dimensions? Array implementations prefer to invoke general
rules to answer such questions. Such a rule might say ``dimensions indexed
with scalars are dropped'', or ``trailing dimensions of size one are
dropped'', or ``the rank of the result is the sum of the ranks of the
indexes'' (as in APL).

Here we consider the indexing rules. How to compute shapes of subarrays. How
to deal with singleton dimensions is but a very special case even though
superficially the rules mention them explicitly.

In Julia, such indexing rules are defined in exactly one place and can be
changed later if so desired.

\subsubsection{The need for flexibility in array indexing rules}

Our goal here is a bit unusual: we are not concerned with which rules might
work best, but merely with how they can be specified, so that domain experts
can experiment.

In fact different domains want different things. E.g. in images, each
dimension might be quite different, e.g. time vs. space vs. color, so you
don't want to drop or rearrange dimensions very often.

In practice we may have to reach a consensus on what rules to use, but this
should not be forced by pure implementation convenience.
The point is that in Julia, these are not enforced a priori. Sometimes the
distinctions between the various indexing rules are semantically meaningful
and that's when this flexibility becomes particularly valuable. For example
Tim Holy's image 4-arrays. Quantum mechanics when you average out multiple
indistinguishable particles. $n$-point correlations functions where which $k$
indices you average out defines any number of lower-point $n-k$ point
correlation functions.

%% Instead of the compiler analyzing indexing expressions and determining an
%% answer using hard-coded logic, we would rather implement the behavior in
%% libraries, so that different kinds of arrays may be defined, or so that rules
%% of similar complexity may be defined for other kinds of objects. But these
%% kinds of rules are unusually difficult to implement in libraries. If a library
%% writes out its indexing logic using imperative code, the host language
%% compiler is not likely to be able to analyze it.

Why is it important to be able to express this in a library? No hidden
theories, some sort of consistency. Another reason is heuristic, don't really
know what people want in the future. Put in what people know now, hope that
the next generation will still have the flexibility to modify it to their
needs.

Our dispatch mechanism permits a novel solution. If a multiple dispatch system
supports variadic functions and argument ``splicing'' (the ability to pass a
structure of $n$ values as $n$ separate arguments to a function), then
indexing behavior can be defined as method signatures.

This solution is still a compromise among the factors outlined above, but it
is a new compromise that provides a net-better solution.

%TODO more

\section{Multiple dispatch}

Multiple dispatch (also known as generic functions, or multi-methods) is an
object-oriented paradigm where methods are defined on combinations of data
types (classes), instead of encapsulating methods inside classes. Methods are
grouped into generic functions. A generic function can be applied to several
arguments, and the method with the most specific signature matching the
arguments is invoked.

One can invent examples where multiple dispatch is useful in classic OO domains
such as GUI programming. We might define a method for drawing a label onto
a button as follows (in Julia syntax):

\begin{verbatim}
function draw(target::Button, obj::Label)
    ...
end
\end{verbatim}

In a numerical setting, binary operators are common and we can easily imagine
needing to define special behavior for some combination of two arguments:

\begin{verbatim}
+(x::Real, z::Complex) = complex(x+real(z), imag(z))
\end{verbatim}

But how much more is there? Would we ever need to define a method on
\emph{three} different types at once? Indeed, most language designers and
programmers seem to have concluded that multiple dispatch might be nice, but is
not essential, and the feature is not often used.
%TODO: cite statistic from study of multiple dispatch showing it is lightly used
Perhaps the few cases that seem to need it can be handled using tricks like
Python's \code{\_\_add\_\_} and \code{\_\_radd\_\_} methods.

However, we have come to believe that while multiple dispatch might not
have much impact in other kinds of programming, technical computing is its
killer application. To a large extent, technical computing is characterized
by the prevalence of highly polymorphic, multi-argument operators. Many of
these can be effectively expressed using a multiple dispatch system that also
supports variadic functions.

This combination of features seems straightforward
enough, and yet it permits surprisingly powerful definitions. For example,
consider a variadic \code{sum} function that adds up its arguments. We could
write the following two methods for it (note that in Julia, \code{Real} is
the abstract supertype of all real number types, and \code{Integer} is the
abstract supertype of all integer types):

\begin{verbatim}
sum(xs::Integer...)

sum(xs::Real...)
\end{verbatim}

In the first case, all arguments are integers and so we can use a naive
summation algorithm. In the second case, we know that at least one argument
is not an integer, so we might want to use some form of compensated
summation instead. Notice that these modest method signatures
capture a subtle property (at least one argument is non-integer)
\emph{declaratively}: there is no need to explicitly loop over the arguments
to examine their types. The signatures also provide useful type information:
at the very least, a compiler could know that all argument values inside
the first method are of type \code{Integer}. Yet the type annotations
are not redundant: they are necessary to specify the desired behavior. There
is also no loss of flexibility: \code{sum} can be called with any combination
of number types, as users of dynamic technical computing languages would expect.



\subsection{\texttt{index\_shape}}

Below we define a function \texttt{index\_shape} that computes the shape of a
result array given a series of index arguments. We show three versions, each
implementing a different rule that users in different domains might want:

{\small
\begin{verbatim}
# drop dimensions indexed with scalars
index_shape() = ()
index_shape(i::Real, I...) = index_shape(I...)
index_shape(i, I...) = tuple(length(i), index_shape(I...)...)
\end{verbatim}
}

\noindent Notes: \newline
1. \verb+i::Real+ denotes a scalar which is dropped \newline
2. \verb+I...+ denotes the remainder of the arguments to be parsed next \newline
3. \verb+length+ counts  elements in an array index, thereby flattening \verb+i+
4. The rightmost three dots flatten the tuple

%TODO make the point that recursive peeling is more general than one dimension
%at a time.

{\small
\begin{verbatim}
# drop trailing dimensions indexed with scalars
index_shape(i::Real...) = ()
index_shape(i, I...) = tuple(length(i), index_shape(I...)...)
\end{verbatim}
}

\noindent Notes: \newline
1. \verb+i::Real...+ denotes trailings scalars  which are dropped \newline


{\small
\begin{verbatim}
# rank summing (APL)
index_shape() = ()
index_shape(i, I...) = tuple(size(i)..., index_shape(I...)...)
\end{verbatim}
}


\noindent Notes: \newline
1. \verb+size(i)+ in contrast to \verb+length(i)+ incorporates component index shapes \newline

These definitions are concise, easy to write, and possible for a compiler to
understand fully using straightforward techniques.

%TODO: point out how this combines the ``object part'' and the ``array part''
%into a coherent whole.
%This is really a statement about implementing array semantics in language X
%using language X itself, and in particular using intrinsic features for handling
%objects to also handle arrays.

The result type is determined using only dataflow type inference, plus a rule
for splicing an immediate container (the type of \texttt{f((a,b)...)} is the
type of \texttt{f(a,b)}). Argument list destructuring takes place inside the
type intersection operator used to combine argument types with method
signatures.

This approach does not depend on any heuristics. Each call to
\texttt{index\_shape} simply requires one recursive invocation of type
inference. This process reaches the base case \texttt{()} for these
definitions, since each recursive call handles a shorter argument list (for
less-well-behaved definitions, we might end up invoking a widening operator
instead).

\begin{verbatim}

diverge() = randbool() ? () : tuple(1, diverge()...)

\end{verbatim}

This is an example of indexing behavior that is not amenable to useful static
analysis, since each branch of \code{diverge()} has different types.

%TODO say something about how types of tuples in Julia are defined.

Such code would throw a type error in languages requiring statically
checkableness such as Haskell. But in Julia, this is still allowed just that
the compiler may not have useful information from static analysis and so may
not run as fast. In Repa, the top priority is to appease the type system of
Haskell, with performance and user interface secondary. We think it should be
the other way round.

\section{Extensions of the idea}

Multiple dispatch can also be used in other contexts.

\subsection{Array views (Dahua's stuff)}

Also interesting are array views. In certain cases of subarray slicing, it is
possible to keep the data in place and return just a view (pointer) to the
data instead of creating a new copy in memory. In this case the same
infrastructure that applies to full arrays automatically works for subarrays.
Views are just another thing that implement the array protocol: (length, size,
getindex). Is there a point here about code reuse? Maybe but Jeff thinks it's
not crucial.

\subsection{Distributed Arrays}

\subsection{Quantities with Units}

The implementation of unitful computations in Julia using the
\code{SIUnits.jl}\cite{Fischer:2014si} package makes use of the same kind of
tradeoffs as for array semantics. Unitful computations are another case
where the relevent metadata (exponents on units) can be
known at compile time in many cases, but not always. Therefore the
\code{SIUnits} library is free to express the general case, and have the
overhead of tagging and dispatching removed where possible.



\section{Conclusions}

A well-designed language force programmers to write code in ways that are
amenable to static analysis. Technically speaking you can say that the
inability to do static analysis is a weakness of compiler implementation, but
this is also a point about the ``power'' of a language. Compare Julia to the
past: language abstraction mechanisms were not powerful enough to define
$n$-arrays. The behaviors had to be built in to the compiler (or inside a very
elaborate C library like NumPy). In Julia, method signatures plus a general-
purpose (not array-specific) type inference algorithm can do it. The
combination turns out to be very important: if you only have multiple
dispatch, you just have a nifty-but-slow way to write these things. If you
just have type inference, then you are quite limited in what you can
realistically infer --- for example an explicit loop over the arguments to
\code{getindex} is too dynamic, and you can't really infer what it does.

Languages have to compromise between static (compiler) analysis - accept more
restrictions so that you can do more static analyses - vs user ease of writing
programs. Other dynamic languages lack extensive static analysis capabilities
in their default implementations. Implementations with some modicum of static
analysis can result in large performance gains, like FALCON or Hack. But in
Julia, we are going one step further and turning the combination of multiple
dispatch and automatic type inference from a performance optimization hack,
into a genuine feature of the programming language, and designing a
programming language to fully leverage the power of this combination.
Automatic type inference reduces the need for manual, tedious type annotation
and reduced redundancy in code. At the same time, types in dispatch signatures
give you access to behavior specialization through multiple dispatch, thus
enabling better performance.

In contrast with statically typed languages like Haskell, Julia by virtue of
being a dynamic language can take a more liberal view on type correctness.
While Julia makes every attempt to infer type information from static
analysis, Julia does not create any issues over type correctness, but rather
execute code anyway in the absence of detailed type information. Contrast this
with the behavior of C++, which is a static language with separate static and
dynamic semantics. C++ resolves function overloads at compile time; this isn't
something you can do at runtime.

Automatic type inference allows for a lot of static analysis without a static
type system. Julia says we do a lot statically without a static type system.
The design idea is that languages with static types assert that the latter are
necessary for correctness. But it is also possible that with static analysis
that the code can be more performant due to specialization.

Julia is sufficiently expressive such that you can write code that in general
may not be amenable to static analysis, but in many cases the compiler is able
to analyze it so that you are in the fast (and statically analyzable)
quadrant. Making it EASIER to move into this quadrant, part of this is because
it is a single language. Sufficiently expressive to write complex, generic,
dynamic multidimensional behaviors, but sufficiently analyzable to know when
it's in one of the cases that C++ or Haskell can express and accordingly
generate efficient code.

Multiple dispatch changes the ``statically analyzable + fast'' versus ``not
analyzable + slow'' trade-off from a property of the language to a property of
a program written in that language. It is a remarkably efficient way to enable
the expressiveness ability to write dynamic code that can nonetheless be
analyzed statically in sufficiently many cases to attain performance. Multiple
dispatch is not just an optimization hack, but can be used in an unprecedented
way to address the performance---flexibility compromise that we believe
achieves greater Pareto optimality than other existing solutions to this
trade-off.

%\appendix
%\section{Appendix Title}
%
%This is the text of the appendix, if you need one.

\acks

The authors gratefully acknowledge the enthusiastic participation of the Julia
developer community for many stimulating discussions on the topic of array
implementations in the Julia language.   We gratefully acknowledge
support from the MIT Deshpande Center, an Intel Science and Technology award,
a grant from VMWare, Citibank, a Horizontal Software Fellowship in Compuational Engineering,  and NSF DMS-1035400.



%TODO thank Intel, Wade, and perhaps NSF Solar here

% We recommend abbrvnat bibliography style.

\bibliography{refs}{}
\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

%\begin{thebibliography}{}
%\softraggedright

%\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
%P. Q. Smith, and X. Y. Jones. ...reference text...

%\end{thebibliography}

\end{document}
