\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:
% preprint      Remove this option only once the paper is in final form.

% Alan TODO:  Can we actually see some Julia arrays, in their full flexibility

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}

\newcommand{\ALGOL}{A\textsc{LGOL}}
\newcommand{\MATLAB}{\textsc{MATLAB}}
\newcommand{\Mathematica}{\textit{Mathematica}}
\newcommand{\code}[1]{\texttt{#1}}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{ARRAY '14}{June 15, 2014, Edinburgh, UK}
\copyrightyear{2014} 
%\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
%\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{Array Operators Using Multiple Dispatch}    % These are ignored unless
\preprintfooter{Array Operators Using Multiple Dispatch} % 'preprint' option specified.

% \title{Array implementations in Julia}

%\title{Array Operators Using Multiple Dispatch}
%\subt
\title{ Array Operators Using Multiple Dispatch }
\subtitle{A design methodology for array implementations in dynamic languages}

\authorinfo{Jeff Bezanson \and Jiahao Chen \and Stefan Karpinski \and Viral Shah  \and Alan Edelman}
           {MIT Computer Science and Artificial Intelligence Laboratory}
  { bezanson@mit.edu, jiahao@mit.edu, stefan@karpinski.org, viral@mayin.org, edelman@mit.edu }

%TODO Get Dahua to sign on as coauthor
%\authorinfo{Dahua Lin}
%           {Toyota Technological Institute at Chicago}
%           {dhlin@ttic.edu}

\maketitle

\begin{abstract}

Arrays are such a rich and fundamental data type that they tend to be built in to
a language, either in the compiler or in a large low-level library.
% TODO: a sentence about why this is not good and why we need more flexibility
Only a few languages, such as C++ and Haskell, provide the necessary power to define
n-dimensional arrays, but these systems rely on compile-time abstraction to provide
performance, sacrificing some amount of flexibility.
In contrast, dynamic languages make it straightforward for the user to define any
behavior they might want, but at the possible expense of performance.

As part of the Julia language project, we have developed an approach that yields
a novel trade-off between flexibility and compile-time analysis. The core
abstraction we use is multiple dispatch. This has been extensively studied
as a general object-oriented programming technique, but we find it
especially relevant to technical and array-based programming. By expressing
key functions such as array indexing using multi-method
signatures, a surprising range of behaviors can be obtained, in a way that is
both relatively easy to write and amenable to compiler analysis.
The compact factoring of concerns provided by these methods makes it easier
for user-defined types to behave consistently with types in the standard
library.


%pulls out abstractions that might not have been named before
%creates better integration of user-defined types
%flexible enough to change the behavior if you want
%creates more coherence

%Interest in recovering performance in these systems has spurred development of
%analysis frameworks. 

\end{abstract}

%\category{CR-number}{subcategory}{third-level}

\keywords
Julia, multiple dispatch, type inference, array indexing, static analysis,
dynamic dispatch

\begin{quotation}
``Unfortunately, it is very difficult for a designer to select in advance all
the abstractions which the users of his language might need. If a language is
to be used at all, it is likely to be used to solve problems which its
designer did not envision, and for which abstractions embedded in the language
are not sufficient.'' - Ref. \cite{Liskov:1974pb}
\end{quotation}

\section{Static analysis is necessary for efficient array implementations}

$n$-arrays (arrays of rank $n$, or simply `arrays') are an essential data
structure for technical computing, but are challenging to implement
efficiently \cite{Sattley:1960as,Sattley:1961as, Randell:1964a6}. There is a
long history of special-purpose compiler optimizations to make operations over
arrays efficient, such as loop fusion for array traversals and common
subexpression elimination for indexing operations \cite{Randell:1964a6,
Busam:1969oe}. Many language implementations therefore choose to build array
semantics into compilers.

However, only a few of the languages that support $n$-arrays have sufficient
power to express the semantics of $n$-arrays for general rank $n$ without
resorting to hard coding array behavior into a compiler written in a different
language. These languages support powerful and efficient libraries
implementing array semantics. Notable examples  include the C++ libraries
\code{Blitz++} \cite{Veldhuizen:1998ab} or \code{Boost.MultiArray}
\cite{Garcia:2005ma}, and the Haskell Repa (Regular Parallel Arrays) library
\cite{Keller:2010rs,Lippmeier:2011ep, Lippmeier:2012gp}. These libraries, at
their core, leverage powerful static semantics of their host langangues to
define $n$-arrays inductively as the outer product of a 1-array with an
$(n-1)$-array \cite{Bavestrelli:2000ct}. This recursive peeling of rank thus
defines general $n$-arrays. The dispatch semantics also allows for efficient
implementations of certain operations; for example, \code{Repa} allows for in-
memory transposition by dispatching on classes with different memory striding
rules \cite{Keller:2010rs}. The use of compile-time features allows these
libraries to eliminate runtime overhead for better performance.

Statically defined array libraries, despite their notable advantages,
nevertheless still have some limitations. First, the language semantics
required to achieve the necessary flexibility of expression and performance,
such as C++ expression templates, are only available at compile time.
Consequently, these libraries would not be able to support programs using
$n$-arrays where $n$ is known only at runtime. Second, efficiency necessitates
the use of notoriously abstruse language features like C++ template
metaprograms. Third, the recursive peeling strategy for defining $n$-arrays
naturally favors only a limited set of indexing behaviors. For example,
\code{Repa}'s reductions like \code{Sum} are only defined naturally over the
last index \cite{Keller:2010rs}; computations over other indices require extra
permutation operations and are not guaranteed to produce an optimal memory
traversal pattern. The overhead incurred by general indexing operations can be
avoided only by implementing many special cases that ensure maximal
exploitation of loop unrolling optimizations and other similar static analyses
\cite{Garcia:2005ma, Lippmeier:2011ep}, which engenders much repetition in the
codebase \cite{Lippmeier:2012gp}.

More importantly, some applications call for semantics that are not amenable
to static analysis. Certain applications require arrays whose ranks are known
only at runtime, and thus the data structures in these program cannot be
guaranteed to fit in a constant amount of memory. Other programs may require
different semantics for arrays of different ranks. Some technical computing
applications, for example, wish to treat vectors (1-arrays) differently from
matrices (2-arrays), supporting computations such as the singular value
decomposition for the latter but not the former. In such contexts, column
vectors are no longer equivalent to $N\times1$ matrices or 2-arrays with a
trailing singleton dimension, and programs that conflate them violate the
Liskov substitution principle \cite{Liskov:1987da}, in ways are are analogous
to the canonical circle-ellipse problem \cite{Halbert:1987ut}.

Applications requiring full runtime flexibility are better expressed in
dynamic languages such as \Mathematica, \MATLAB, R, and Python/NumPy, where
most, if not all, operations will be dynamically dispatched. Such flexibility,
however, has traditionally come at the price of slower performance.
Array implementations of dynamic languages striving for better performance have
traditionally resorted to strategies that intimately involve static analyses. One strategy is to implement arrays in an external library
written in a static language. A prominent example of this strategy is the
Python NumPy package, which implements special static analyses and dispatch
mechanisms within a large C code base. As a result, NumPy \code{ndarray}s are
superficially Python objects, but implementation-wise are disjoint from the
rest of the Python object system, since little of Python's native object
semantics are used to define their behavior.

Another possibility for performant arrays is to implement static analyses
\textit{de novo} for the dynamic language. However, the greater flexibility
allowed in arbitrary programs limit the extent of static analyses based on
finite-height partial ordering over patterns. For example, \MATLAB's array
semantics allow an array to be enlarged automatically whenever a write occurs
to an out-of-bounds reference, and also for certain operations to
automatically promote the element type of an array from real to complex
numbers. This poses implementation challenges for static \MATLAB{} compilers
like \code{FALCON}, which have to implement a complete type system with
multiple compiler passes and complex interprocedural flow analyses to check
specifically for such drastic runtime changes to arrays \cite{Rose:1999tt,
Li:2013mf}. Additionally, type checking was essential to disambiguate
\MATLAB{} expressions like \code{A*B}, which, depending on the dimensions of
\code{A} and \code{B}, could represent a scaling, inner product, outer
product, matrix-matrix multiplication, or matrix-vector multiplication
\cite{Rose:1999tt}. Similar challenges exist in other languages like Octave
\cite{Eaton:2001op} and Hack, a PHP implementation which contains a full
static type checking system \cite{Verlaguet:2014hn}.

%TODO other languages: APL, ZPL, Lisp?

The conflicting requirements of performance and flexibility pose a dilemma for
language designers and implementers. Most current languages choose either to
support only programs that are amenable to static analysis for the sake of
performance, such as C++ and Haskell, or choose to support more general
classes of programs, like \MATLAB, \Mathematica, Python/NumPy, and APL. While
dynamic languages nominally give up static analysis in the process, many
implementations of these languages still resort to static analysis in
practice, either by hard-coding array semantics in a compiler, or by
implementing arrays in an external library written in a static language. Thus
we see that having some modicum of static analysis is unavoidable for
reasonable performance.

A key problem with dynamic languages is how to provide more declarative
information to a compiler. Users expect to be able to program any
run-time behavior and have it just work. In theory, a compiler could
partially evaluate programs based on whatever static information it can
find (e.g. integer constants), but it is unlikely that this would reliably
provide performance where it is needed.

%The user should not have to learn two different sets of semantic behavior. In Julia, that's the compiler's job. It does mean that the it's not as fast as a static language.

\section{Julia Arrays}

Arrays are implemented in Julia\cite{Bezanson:2012jf} as the \code{Array}
data type, which is parameterized by an element type and a rank (an integer).
For purposes of this paper, an \code{Array} can be considered a tuple of
a contiguous memory region and a shape (a tuple of integers giving the size
of the array in each dimension). This simple representation is already
enough to require nontrivial design decisions.

%% Multiple dispatch makes it possible for \code{Array}s to have implementations
%% that afford both generality and efficiency. The most general
%% \code{Array\{Any\}} is an array of pointers to heap-allocated values. However,
%% \code{Array}s of native machine types like double precision floating point
%% numbers (\code{Float64}s) or integers of native machine size (\code{Int}s) can
%% be implemented to be C/Fortran- compatible. Interestingly,
%% \code{Array\{Nothing\}} can be implemented with zero storage space at runtime,
%% since it consists of elements of an immutable type with zero fields, which is
%% possible because only a single instance of any such type can exist, and hence
%% need not actually be stored. This allows for some clever tricks like
%% implementing \code{Set\{T\}} using a \code{Dict\{T,Nothing\}} to avoid paying
%% any cost for the value array in the \code{Dict}.

%TODO identify subthemes in this giant block
The implementation of Julia \code{Array}s differs significantly from other
languages in several important respects. First, automatic type inference is
not used merely as a compiler optimization technique, like in static compilers
for dynamic languages, but instead is a crucial language feature that, in
conjunction with multiple dispatch, are necessary for implement language
semantics. Second, Julia by virtue of being a dynamic language provides the
static analyses enabled by type inference and multiple dispatch at runtime. In
contrast, the C++ expression templates are static semantics that are
unavailable at runtime, and the semantics of templates are for the purposes of
runtime programs effectively a separate language. The single all- runtime
model of Julia is much easier to use and imposes fewer restrictions on the
kind of Julia code that can be written. You can implement a bigger class of
rules in Julia rather than having to special case on the number of dimensions
or be forced to rely on recursive definitions onto subarrays. For example in
Julia you can do \code{A[I...]} where \code{I} is a heterogeneous array, and
all the same definitions are still applicable, but you might take a bit of
performance drop. Third, Julia aggressive employs specialization on argument
types by default, generating a large variety of specialized methods for each
generic function, in contrast with the default of one compilation per function
declaration in most other languages. This is possible thanks to the static
analyses enabled by type inference and multiple dispatch. C++ can do this with
templates, but generating an entire family of related methods must be
requested explicitly using templates. Furthermore, having all the semantics
enabled by static analysis at runtime allows for composition for code that
results in efficient specialized methods by the compiler. For example, Julia
code implementing the algebra of the quaternion field can be composed
transparently with other Julia code implementing Gaussian elimination by the
compiler. One can be inlined into the other and woven into a specialized
method for Gaussian elimination on quaternionic matrices. Contrast this with
the conventional wisdom that user-defined types are slow in languages like
\MATLAB{} and even C++; user-defined classes are reference types rather than
value types, and methods have to be declared \code{virtual} to enable dynamic
dispatch as opposed to the default static dispatch mechanism. In Julia,
everything in principle has to happen dynamically but then the focus is on
analyzing it statically. Fourth, array functions in Julia have access to the
entire argument list, allowing for conceptual models and APIs that are not
restricted to strict left-to-right or right-to-left index traversal that would
be necessary by recursive peeling. Fifth, specialization of methods can be
implemented with multiple dispatch, which is a powerful alternative to the
single dispatch mechanism of Haskell based on function classes and pattern
matching.

In the cases where static analysis can happen, Julia can perform as fast as
languages where array semantics is predicated on static analysis, but multiple
dispatch allows Julia to be more flexible than just this. Julia also allows
for generic fall-back methods that are slower because of a lack of static
analysis, but nonetheless can be dispatched upon. The flexibility that is
allowed by this approach is a very nonobvious application of multiple
dispatch.

\subsection{Array indexing rules}

Whatever decision is made, rules must be defined for how various operators act
on dimensions. For now we will focus on indexing, since selecting parts of
arrays has particularly rich behavior with respect to dimensionality. For
example, if a single row or column of a matrix is selected, does the result
have one or two dimensions? Array implementations prefer to invoke general
rules to answer such questions. Such a rule might say ``dimensions indexed
with scalars are dropped'', or ``trailing dimensions of size one are
dropped'', or ``the rank of the result is the sum of the ranks of the
indexes'' (as in APL).

Here we consider the indexing rules. How to compute shapes of subarrays. How
to deal with singleton dimensions is but a very special case even though
superficially the rules mention them explicitly.

In Julia, such indexing rules are defined in exactly one place and can be
changed later if so desired.

\subsubsection{The need for flexibility in array indexing rules}

Our goal here is a bit unusual: we are not concerned with which rules might
work best, but merely with how they can be specified, so that domain experts
can experiment.

In fact different domains want different things. E.g. in images, each
dimension might be quite different, e.g. time vs. space vs. color, so you
don't want to drop or rearrange dimensions very often.

In practice we may have to reach a consensus on what rules to use, but this
should not be forced by pure implementation convenience.
The point is that in Julia, these are not enforced a priori. Sometimes the
distinctions between the various indexing rules are semantically meaningful
and that's when this flexibility becomes particularly valuable. For example
Tim Holy's image 4-arrays. Quantum mechanics when you average out multiple
indistinguishable particles. $n$-point correlations functions where which $k$
indices you average out defines any number of lower-point $n-k$ point
correlation functions.

%% Instead of the compiler analyzing indexing expressions and determining an
%% answer using hard-coded logic, we would rather implement the behavior in
%% libraries, so that different kinds of arrays may be defined, or so that rules
%% of similar complexity may be defined for other kinds of objects. But these
%% kinds of rules are unusually difficult to implement in libraries. If a library
%% writes out its indexing logic using imperative code, the host language
%% compiler is not likely to be able to analyze it.

Why is it important to be able to express this in a library? No hidden
theories, some sort of consistency. Another reason is heuristic, don't really
know what people want in the future. Put in what people know now, hope that
the next generation will still have the flexibility to modify it to their
needs.

Our dispatch mechanism permits a novel solution. If a multiple dispatch system
supports variadic functions and argument ``splicing'' (the ability to pass a
structure of $n$ values as $n$ separate arguments to a function), then
indexing behavior can be defined as method signatures.

This solution is still a compromise among the factors outlined above, but it
is a new compromise that provides a net-better solution.

%TODO more

\section{Multiple dispatch}

Multiple dispatch (also known as generic functions, or multi-methods) is an
object-oriented paradigm where methods are defined on combinations of data
types (classes), instead of encapsulating methods inside classes. Methods are
grouped into generic functions. A generic function can be applied to several
arguments, and the method with the most specific signature matching the
arguments is invoked.

One can invent examples where multiple dispatch is useful in classic OO domains
such as GUI programming. We might define a method for drawing a label onto
a button as follows (in Julia syntax):

\begin{verbatim}
function draw(target::Button, obj::Label)
    ...
end
\end{verbatim}

In a numerical setting, binary operators are common and we can easily imagine
needing to define special behavior for some combination of two arguments:

\begin{verbatim}
+(x::Real, z::Complex) = complex(x+real(z), imag(z))
\end{verbatim}

But how much more is there? Would we ever need to define a method on
\emph{three} different types at once? Indeed, most language designers and
programmers seem to have concluded that multiple dispatch might be nice, but is
not essential, and the feature is not often used.
%TODO: cite statistic from study of multiple dispatch showing it is lightly used
Perhaps the few cases that seem to need it can be handled using tricks like
Python's \code{\_\_add\_\_} and \code{\_\_radd\_\_} methods.

However, we have come to believe that while multiple dispatch might not
have much impact in other kinds of programming, technical computing is its
killer application. To a large extent, technical computing is characterized
by the prevalence of highly polymorphic, multi-argument operators. Many of
these can be effectively expressed using a multiple dispatch system that also
supports variadic functions.

This combination of features seems straightforward
enough, and yet it permits surprisingly powerful definitions. For example,
consider a variadic \code{sum} function that adds up its arguments. We could
write the following two methods for it (note that in Julia, \code{Real} is
the abstract supertype of all real number types, and \code{Integer} is the
abstract supertype of all integer types):

\begin{verbatim}
sum(xs::Integer...)

sum(xs::Real...)
\end{verbatim}

In the first case, all arguments are integers and so we can use a naive
summation algorithm. In the second case, we know that at least one argument
is not an integer, so we might want to use some form of compensated
summation instead. Notice that these modest method signatures
capture a subtle property (at least one argument is non-integer)
\emph{declaratively}: there is no need to explicitly loop over the arguments
to examine their types. The signatures also provide useful type information:
at the very least, a compiler could know that all argument values inside
the first method are of type \code{Integer}. Yet the type annotations
are not redundant: they are necessary to specify the desired behavior. There
is also no loss of flexibility: \code{sum} can be called with any combination
of number types, as users of dynamic technical computing languages would expect.



\subsection{\texttt{index\_shape}}

Below we define a function \texttt{index\_shape} that computes the shape of a
result array given a series of index arguments. We show three versions, each
implementing a different rule that users in different domains might want:

{\small
\begin{verbatim}
# drop dimensions indexed with scalars
index_shape() = ()
index_shape(i::Real, I...) = index_shape(I...)
index_shape(i, I...) = tuple(length(i), index_shape(I...)...)
\end{verbatim}
}

\noindent Notes: \newline
1. \verb+i::Real+ denotes a scalar which is dropped \newline
2. \verb+I...+ denotes the remainder of the arguments to be parsed next \newline
3. \verb+length+ counts  elements in an array index, thereby flattening \verb+i+
4. The rightmost three dots flatten the tuple

%TODO make the point that recursive peeling is more general than one dimension
%at a time.

{\small
\begin{verbatim}
# drop trailing dimensions indexed with scalars
index_shape(i::Real...) = ()
index_shape(i, I...) = tuple(length(i), index_shape(I...)...)
\end{verbatim}
}

\noindent Notes: \newline
1. \verb+i::Real...+ denotes trailings scalars  which are dropped \newline


{\small
\begin{verbatim}
# rank summing (APL)
index_shape() = ()
index_shape(i, I...) = tuple(size(i)..., index_shape(I...)...)
\end{verbatim}
}


\noindent Notes: \newline
1. \verb+size(i)+ in contrast to \verb+length(i)+ incorporates component index shapes \newline

These definitions are concise, easy to write, and possible for a compiler to
understand fully using straightforward techniques.

%TODO: point out how this combines the ``object part'' and the ``array part''
%into a coherent whole.
%This is really a statement about implementing array semantics in language X
%using language X itself, and in particular using intrinsic features for handling
%objects to also handle arrays.

The result type is determined using only dataflow type inference, plus a rule
for splicing an immediate container (the type of \texttt{f((a,b)...)} is the
type of \texttt{f(a,b)}). Argument list destructuring takes place inside the
type intersection operator used to combine argument types with method
signatures.

This approach does not depend on any heuristics. Each call to
\texttt{index\_shape} simply requires one recursive invocation of type
inference. This process reaches the base case \texttt{()} for these
definitions, since each recursive call handles a shorter argument list (for
less-well-behaved definitions, we might end up invoking a widening operator
instead).

\begin{verbatim}

diverge() = randbool() ? () : tuple(1, diverge()...)

\end{verbatim}

This is an example of indexing behavior that is not amenable to useful static
analysis, since each branch of \code{diverge()} has different types.

%TODO say something about how types of tuples in Julia are defined.

Such code would throw a type error in languages requiring statically
checkableness such as Haskell. But in Julia, this is still allowed just that
the compiler may not have useful information from static analysis and so may
not run as fast. In Repa, the top priority is to appease the type system of
Haskell, with performance and user interface secondary. We think it should be
the other way round.

\section{Extensions of the idea}

Multiple dispatch can also be used in other contexts.

\subsection{Array views (Dahua's stuff)}

Also interesting are array views. In certain cases of subarray slicing, it is
possible to keep the data in place and return just a view (pointer) to the
data instead of creating a new copy in memory. In this case the same
infrastructure that applies to full arrays automatically works for subarrays.
Views are just another thing that implement the array protocol: (length, size,
getindex). Is there a point here about code reuse? Maybe but Jeff thinks it's
not crucial.

\subsection{Distributed Arrays}

\subsection{Quantities with Units}

The implementation of unitful computations in Julia using the
\code{SIUnits.jl}\cite{Fischer:2014si} package makes use of the same kind of
tradeoffs as for array semantics. Unitful computations are another case
where the relevent metadata (exponents on units) can be
known at compile time in many cases, but not always. Therefore the
\code{SIUnits} library is free to express the general case, and have the
overhead of tagging and dispatching removed where possible.



\section{Conclusions}

A well-designed language force programmers to write code in ways that are
amenable to static analysis. Technically speaking you can say that the
inability to do static analysis is a weakness of compiler implementation, but
this is also a point about the ``power'' of a language. Compare Julia to the
past: language abstraction mechanisms were not powerful enough to define
$n$-arrays. The behaviors had to be built in to the compiler (or inside a very
elaborate C library like NumPy). In Julia, method signatures plus a general-
purpose (not array-specific) type inference algorithm can do it. The
combination turns out to be very important: if you only have multiple
dispatch, you just have a nifty-but-slow way to write these things. If you
just have type inference, then you are quite limited in what you can
realistically infer --- for example an explicit loop over the arguments to
\code{getindex} is too dynamic, and you can't really infer what it does.

Languages have to compromise between static (compiler) analysis - accept more
restrictions so that you can do more static analyses - vs user ease of writing
programs. Other dynamic languages lack extensive static analysis capabilities
in their default implementations. Implementations with some modicum of static
analysis can result in large performance gains, like FALCON or Hack. But in
Julia, we are going one step further and turning the combination of multiple
dispatch and automatic type inference from a performance optimization hack,
into a genuine feature of the programming language, and designing a
programming language to fully leverage the power of this combination.
Automatic type inference reduces the need for manual, tedious type annotation
and reduced redundancy in code. At the same time, types in dispatch signatures
give you access to behavior specialization through multiple dispatch, thus
enabling better performance.

In contrast with statically typed languages like Haskell, Julia by virtue of
being a dynamic language can take a more liberal view on type correctness.
While Julia makes every attempt to infer type information from static
analysis, Julia does not create any issues over type correctness, but rather
execute code anyway in the absence of detailed type information. Contrast this
with the behavior of C++, which is a static language with separate static and
dynamic semantics. C++ resolves function overloads at compile time; this isn't
something you can do at runtime.

Automatic type inference allows for a lot of static analysis without a static
type system. Julia says we do a lot statically without a static type system.
The design idea is that languages with static types assert that the latter are
necessary for correctness. But it is also possible that with static analysis
that the code can be more performant due to specialization.

Julia is sufficiently expressive such that you can write code that in general
may not be amenable to static analysis, but in many cases the compiler is able
to analyze it so that you are in the fast (and statically analyzable)
quadrant. Making it EASIER to move into this quadrant, part of this is because
it is a single language. Sufficiently expressive to write complex, generic,
dynamic multidimensional behaviors, but sufficiently analyzable to know when
it's in one of the cases that C++ or Haskell can express and accordingly
generate efficient code.

Multiple dispatch changes the ``statically analyzable + fast'' versus ``not
analyzable + slow'' trade-off from a property of the language to a property of
a program written in that language. It is a remarkably efficient way to enable
the expressiveness ability to write dynamic code that can nonetheless be
analyzed statically in sufficiently many cases to attain performance. Multiple
dispatch is not just an optimization hack, but can be used in an unprecedented
way to address the performance---flexibility compromise that we believe
achieves greater Pareto optimality than other existing solutions to this
trade-off.

%\appendix
%\section{Appendix Title}
%
%This is the text of the appendix, if you need one.

\acks

The authors gratefully acknowledge the enthusiastic participation of the Julia
developer community for many stimulating discussions on the topic of array
implementations in the Julia language, in particular Dahua Lin and Keno
Fischer for the \code{ArrayViews.jl}\cite{Lin:2014av} and
\code{SIUnits.jl}\cite{Fischer:2014si} packages respectively. Funding for this
work was supported from the MIT Deshpande Center, an Intel Science and
Technology award, a grant from VMWare, Citibank, a Horizontal Software
Fellowship in Compuational Engineering, and NSF DMS-1035400.

% We recommend abbrvnat bibliography style.

\bibliography{refs}{}
\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

%\begin{thebibliography}{}
%\softraggedright

%\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
%P. Q. Smith, and X. Y. Jones. ...reference text...

%\end{thebibliography}

\end{document}
