\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:
% preprint      Remove this option only once the paper is in final form.

% Alan TODO:  Can we actually see some Julia arrays, in their full flexibility

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}

\newcommand{\ALGOL}{A\textsc{LGOL}}
\newcommand{\MATLAB}{\textsc{MATLAB}}
\newcommand{\Mathematica}{\textit{Mathematica}}
\newcommand{\code}[1]{\texttt{#1}}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{ARRAY '14}{June 15, 2014, Edinburgh, UK}
\copyrightyear{2014} 
%\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
%\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{Array Operators Using Multiple Dispatch}    % These are ignored unless
\preprintfooter{Array Operators Using Multiple Dispatch} % 'preprint' option specified.

% \title{Array implementations in Julia}
% \subtitle{Implementing arrays in a way that is amenable to compiler analysis}

%\title{Array Operators Using Multiple Dispatch}
%\subt
\title{ A design methodology for array implementations in dynamic languages }

\authorinfo{Jeff Bezanson \and Jiahao Chen \and Keno Fischer \and Stefan Karpinski \and Viral Shah  \and Alan Edelman}
           {MIT Computer Science and Artificial Intelligence Laboratory}
  { bezanson@csail.mit.edu, jiahao@mit.edu, kfischer@college.harvard.edu, stefan@karpinski.org, viral@mayin.org, edelman@mit.edu }
           
     
%TODO Get Dahua to sign on as coauthor
%\authorinfo{Dahua Lin}
%           {Toyota Technological Institute at Chicago}
%           {dhlin@ttic.edu}

\maketitle

\begin{abstract}

Arrays, the most basic of data structures, tends to be built in to a language early
either in the compiler or a large low-level library. 
More recent developments
beg for more flexibility and generality, as
modern array programming systems are typically integrated with object-oriented
languages.  A consequence of the traditional approach is that the array data type is difficult to
modify or extend, and is even unnecessarily more difficult to implement.

% ??  [The below delays the good point]
%Arguably, efficient
%and flexible n-d arrays are one of the more difficult datatypes to implement
% at the library level.

In this work, we present a new approach, developed as part of 
the Julia language project, where the object system is used to define the behaviors
of  \framebox{  {\bf key array operators} } \marginpar{Which operators?  Which functionality?  reshape? indexing? others? too vague}  --- not just to organize the 
\framebox{  {\bf functionality} } into
classes.
The combination of flexible multi-method
signatures and dataflow type inference yields a novel trade-off between
flexibility and compiler analysis.

\framebox{The words ``multiple dispatch'', should appear in the abstract?}
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

\keywords
Julia, multiple dispatch, type inference, array indexing, static analysis,
dynamic dispatch

\section{Introduction}

\begin{quotation}
``Unfortunately, it is very difficult for a designer to select in advance all
the abstractions which the users of his language might need. If a language is
to be used at all, it is likely to be used to solve problems which its
designer did not envision, and for which abstractions embedded in the language
are not sufficient.'' - Ref. \cite{Liskov:1974pb}
\end{quotation}

%BEGIN OUTLINE

%Introduction to multiple dispatch: traditionally it is considered an optimization

%Julia philosophy: Instead of MD as a hack, turn it around and make it a core feature of the language.

%In this paper we show how to use MD to implement semantic features.

A particularly interesting use case is $n$-arrays (arrays of rank $n$, or
simply `array'). While 1-arrays are an essential data type in many programming
languages, multidimensional arrays in their full generality are significantly
more challenging to implement efficiently \cite{Sattley:1960as,
Sattley:1961as, Randell:1964a6}. For example, programs that traverse arrays
require many indexing computations, and for these programs to perform
efficiently, there is a strong incentive for programming languages to
implement static analysis such as constant subexpression elimination and loop
fusion/reordering of array traversals \cite{Busam:1969oe}.

Using arrays an example, we can classify languages into those favoring
static analysis and those favoring full dynamic generality.

\subsection{Languages favoring static analysis}

Static languages like Fortran, C/C++ and Haskell fall naturally into this
category. In extreme cases, some static languages only support $n$-arrays up
to some fixed maximal rank, like Fortran.\footnote{The maximal rank was 3 in
FORTRAN I \cite{Backus:1957fa}, increased to 7 in Fortran 77, and later to 15
in Fortran 2008.} The advantage of imposing rank limits is that specialized
implementations for each rank $n$ can be written out in full. The
disadvantage, of course, is that program expressibility is limited and it is
challenging to write libraries for general $n$-arrays in these languages.

Most other static languages, however, allow for greater flexibility while
still restricting programs to be amenable to static analysis. A notable
language in this category is C++, whose expression templates are sufficiently
powerful to allow the implementation of $n$-arrays in fast, performant
libraries, like \code{Blitz++} \cite{Veldhuizen:1998ab} or
\code{Boost.MultiArray} \cite{Garcia:2005ma}.  Nevertheless, template
metaprograms are nonetheless notoriously difficult to implement and
comprehend. While $n$-arrays can be implemented compactly using recursive
template metaprograms \cite{Bavestrelli:2000ct}, peeling off $(n-1)$-arrays
recursively from $n$-arrays, production quality libraries avoid explicit
recurrences, implementing instead a great many special-cased implementations
of $n$-arrays to ensure that C++ compilers unroll loops fully to achieve
maximal performance \cite{Garcia:2005ma}. The recursive peeling design also
restricts indexing semantics in C++ arrays to traverse only from  leftmost to
rightmost index. Furthermore, C++ expression templates implemented
intrinsically using static (compile-time) semantics which are not accessible
dynamically (at runtime); C++ programs therefore cannot make use of arrays
whose ranks are known only at runtime. . Also you have handle dimensions one
at a time.

Another notable static language that supports $n$-arrays is Haskell, and in
particular its Repa (Regular Parallel Arrays) library \cite{Keller:2010rs}.
One key abstraction is the \code{Shape} class, which enables dispatch on the
index-to-offset mapping function \cite{Keller:2010rs}. This allows for in-
memory representations of transposed arrays, simply by changing the
\code{Shape} class to redefine the memory striding rules \cite{Keller:2010rs}.
Nevertheless, Repa employs the same recursive peeling semantics of C++ array
libraries: for example, reductions like \code{Sum} are only defined
over the last index \cite{Keller:2010rs}. In principle, a generalized
\code{Sum} over an arbitrary index can be composed as \code{Sum} with pre- and
post-permutations to swap the desired index with the last index. However,
this is semantically more work and furthermore isn't guaranteed to produce a
good memory traversal pattern. Similarly, operations over more than one
dimension are implemented recursively over the dimensions, which means that
the compiler has to make the difficult decision whether or not to unroll the
recursion \cite{Lippmeier:2011ep}. Manual unrolling is of course a possibility
but has the cost of introducing repetition in the code
\cite{Lippmeier:2012gp}.

%Arguably, a purely functional language like Haskell, which disallows
%imperative constructs like loops, forces programmers to identify
%abstractions. With fewer options to implement semantics, semantics is more
%intimately tied to types.

\subsection{Languages favoring dynamic generality}

Conversely, some applications may call for semantics that are not amenable to
static analysis. For example, one can envision applications requiring dynamic
arrays whose ranks are known only at runtime. Languages that wish to support
such applications of some applications cannot choose only to use data
structures that fit in a constant amount of memory. Another possibility is
that programs may wish to treat arrays with different numbers of dimensions
very differently. A vector---an intrinsically one-dimensional object---can be
conceptually very different from an $N\times1$ matrix, which is intrinsically
two-dimensional. For example, many linear algebraic computations like the
singular value decomposition make sense only for matrices, not vectors. These
differences in object semantics and behavior make the number of dimensions a
crucial part of program semantics and thus cannot be merely a compiler
implementation detail. Even though it is possible to construct an isomorphism
between column vectors and $N\times1$ matrices, semantic differences preclude
a genuine ``is-a'' relationship, and hence results in a violation of the
Liskov substitution principle \cite{Liskov:1987da}. We can think of this
situation as the linear algebraic counterpart to the canonical circle-ellipse
problem \cite{Halbert:1987ut}. For such applications requiring different
semantics for different ranks at runtime, runtime flexibility is also essential.

These use cases favor dynamic languages which support full runtime
flexibility in arrays, such as \Mathematica{}, \MATLAB{} and Python/NumPy,
and accept that most or all operations will be dynamically dispatched.

However, languages in this category are not without their pitfalls. First,
array semantics may be implemented using flexible data structure that may
cause abstraction leakage. For example, \Mathematica{} implements $n$-arrays
using nested lists of expressions \cite{mathematica:nl, mathematica:int}, with
the \code{[[]]} (\code{Part}) operator playing the role of array indexing.
However, \Mathematica's powerful pattern matching semantics are prone to
abstraction leakage. For example, it is possible to ``index'' an array with
more indices than its rank; the extra indices would simply traverse into the
expression contained within the matrix element. For example, \code{Array[f,
{3, 4}][[3, 2]]} returns \code{f[3,2]}, whilst \code{Array[f, {3, 4}][[3, 2,
1]]} returns \code{3}. Thus the semantics of the \code{Part} symbol is more
general than pure array indexing and allows for violations of the array data
abstraction, which may come as a surprise to users. Another example in
\MATLAB{} comes from implementing arrays of user-defined classes. Since
\MATLAB{} arrays are not defined in terms of its object system, special care
must be taken for arrays of user-defined classes to have the same semantic
behavior as arrays of built- in types. In principle, user-defined classes
could otherwise lead to arbitrary behavior.

% Jeff calls this "just overloading".

Second, the flexibility of these languages results in generally slow
performance, and language implementations wishing to attain performance must
instead resort to advanced static analyses anyway. For example, \MATLAB's
array semantics allow an array to be enlarged automatically whenever a write
occurs to an out-of-bounds reference, and also for certain operations to
automatically promote the element type of an array from real to complex
numbers. This poses implementation challenges for static \MATLAB{} compilers
like \code{FALCON}, which have to implement a complete type system with
multiple compiler passes and complex interprocedural flow analysis to check
specifically for such drastic runtime changes to arrays \cite{Rose:1999tt,
Li:2013mf}. Additionally, type checking was essential to disambiguate
\MATLAB{} expressions like \code{A*B}, which, depending on the dimensions of
\code{A} and \code{B}, could represent a scaling, inner product, outer
product, matrix- matrix multiplication, matrix-vector multiplication
\cite{Rose:1999tt}. Similar issues also plague the Octave interpreter
\cite{Eaton:2001op}. Even despite these heroic efforts, the flexibility of
languages in this category inevitably limits the extent of static analyses
(based on a finite-height partial order over patterns) that is possible over
an arbitrary program.

Third, implementations of such flexible languages often resort to implementing
arrays in libraries in static languages for the sake of performance. This
leads to a disjunction between the implementation of arrays and the native
object system of the language. The machinery of the Python NumPy library is a
good case in point. NumPy arrays are Python objects, but instead of using the
native Python object system's semantic mechanisms, NumPy instead has its own
abstraction mechanisms implemented with a large C code base for compile-time
code generation and custom runtime dispatch mechanisms. The resulting Python
objects are little more than lookup tables of behaviors, primarily centered
around indexing behavior, and in particular indexing of a single index. In
other languages like R, the disjunction occurs because the array and object
subsystems were added the language at different points in time.

%TODO other languages: R, APL, ZPL?

To summarize, the flexibility of languages in this category can lead to faulty
array abstractions and slow performance. To address performance issues,
language implementations must either reimplement large components of compiler
infrastructure in order to perform static analyses like type inference, or
resort to implementing key components of array infrastructure in external
libraries written in a static language like C or Fortran to achieve acceptable
performance, with the result that the first language becomes little more than
a superficial wrapper.

\subsection{The performance---flexibility trade-off}

The conflicting requirements of performance and flexibility pose a dilemma for
language designers and implementers. Most current languages choose either to
support only programs that are amenable to static analysis for the sake of
performance, such as C++ and Haskell, or choose to support more general
classes of programs, like \MATLAB, \Mathematica, Python/NumPy, and APL. While
dynamic languages nominally give up static analysis in the process, many
implementations of these languages still resort to static analysis in
practice, either by hard-coding array semantics in a compiler, or by
implementing arrays in an external library written in a static language. Thus
we see that having some modicum of static analysis is unavoidable for
reasonable performance.

We propose in this paper that multiple dispatch is not just a particularly
useful kind of static analysis, but instead can be fully leveraged to
implement general array semantics within a single language that nevertheless
allows for reasonable performance in most use cases.

%TODO A static # of dimensions is probably sufficient for most applications,
%but there is a large cliff where everything changes if your code is more
%dynamic

\section{Array indexing}

Whatever decision is made, rules must be defined for how various operators act
on dimensions. For now we will focus on indexing, since selecting parts of
arrays has particularly rich behavior with respect to dimensionality. For
example, if a single row or column of a matrix is selected, does the result
have one or two dimensions? Array implementations prefer to invoke general
rules to answer such questions. Such a rule might say ``dimensions indexed
with scalars are dropped'', or ``trailing dimensions of size one are
dropped'', or ``the rank of the result is the sum of the ranks of the
indexes'' (as in APL).

\section{Julia Arrays}

Julia uses the object system to implement arrays. Arrays are implemented as
the \code{Array} data type, which is parametric on the \code{Type} of element
it contains, and also its rank (which defaults to 1). Internally,
\code{Array}s are implemented as a pair of (shape, linear data storage) =
metadata of bounded size O(1) + raw data O(N). This basic structure is
sufficient to define the semantics of multidimensional arrays, yet is flexible
to incorporate a wide range of nontrivial design decisions.

One of the core features of Julia is multiple dispatch. Consequently, it is
possible for \code{Array}s to have implementations that afford both generality
and efficiency. The most general \code{Array\{Any\}} is an associative array
implemented as an array of pointers to heap-allocated values. However,
\code{Array}s of native machine types like double precision floating point
numbers (\code{Float64}s) or integers of native machine size (\code{Int}s) can
be implemented to be C/Fortran-compatible. Interestingly,
\code{Array\{Nothing\}} can be implemented with zero storage space at runtime,
since it consists of elements of an immutable type with zero fields, which is
possible because only a single instance of any such type can exist, and hence
need not actually be stored. This allows for some clever tricks like
implementing \code{Set\{T\}} using a \code{Dict\{T,Nothing\}} to avoid paying
any cost for the value array in the \code{Dict}.

Contrast the Julia approach to static compilers for other dynamic languages,
e.g. FALCON (a \MATLAB{} static compiler). The latter use dynamic type
inference used to speed up existing language implementation.
\framebox{ Great,} \marginpar{colloquial} but what
you should really do is figure out how to redesign the language given that
technique, to take full advantage of it.

Contrast the Julia approach to the C++ expression template approach. Julia, as
a dynamic language, does not have separate static and dynamic semantics.
Unlike templates, which are effectively a separate language, the single all-
runtime model of Julia is much easier to use and imposes fewer restrictions on
the kind of Julia code that can be written. You can implement a bigger class
of rules in Julia rather than having to special case on the number of
dimensions or be forced to rely on recursive definitions onto subarrays. For
example in Julia you can do `A[I...]` where `I` is a heterogeneous array, and
all the same definitions are still applicable, but you might take a bit of
performance drop.

Contrast the Julia approach to NumPy's. Dynamic dispatch is powerful, but
Julia's approach is much more powerful. We believe that a single powerful
dispatch mechanism can subsume both the compile-time code generation
challenges and custom run-time dynamic dispatch mechanism in NumPy.

Contrast the Julia approach to Haskell/Repa. Statically typed languages tend
to favor recursion down array ranks one at a time, which limit how you can do
operations across multiple dimensions. In Julia we have access to the entire
argument list, which lets you have different mental models and APIs.
Multiple dispatch is also a powerful alternative to the single dispatch
mechanism of Haskell based on function classes and pattern matching, which Repa
uses extensively to implement the many array representations which were
introduced to deal with many different use cases
\cite{Lippmeier:2011ep,Lippmeier:2012gp}.

\subsection{Indexing rules}

Here we consider the indexing rules. How to compute shapes of subarrays. How
to deal with singleton dimensions is but a very special case even though
superficially the rules mention them explicitly.

In Julia, such indexing rules are defined in exactly one place and can be
changed later if so desired.

\subsubsection{The need for flexibility in array indexing rules}

Our goal here is a bit unusual: we are not concerned with which rules might
work best, but merely with how they can be specified, so that domain experts
can experiment.

In fact different domains want different things. E.g. in images, each
dimension might be quite different, e.g. time vs. space vs. color, so you
don't want to drop or rearrange dimensions very often.

In practice we may all have to reach a consensus on what rules to use, but it
should not be enforced upon uses by pure technical implementation convenience.
The point is that in Julia, these are not enforced a priori. Sometimes the
distinctions between the various indexing rules are semantically meaningful
and that's when this flexibility becomes particularly valuable. For example
Tim Holy's image 4-arrays. Quantum mechanics when you average out multiple
indistinguishable particles. $n$-point correlations functions where which $k$
indices you average out defines any number of lower-point $n-k$ point
correlation functions.

\subsection{Assumptions}
Julia's hard assumptions are:

\begin{enumerate}
\item  Array code can not be  manually implemented inside the compiler
\item The compiler must be able to reasonably understand array programs
\item Code must be reasonably easy to write
\end{enumerate}

Instead of the compiler analying indexing expressions and determining an answer
using hard-coded logic, we would rather implement the
behavior in libraries, so that different kinds of arrays may be defined, or so
that rules of similar complexity may be defined for other kinds of objects.
But these kinds of rules are unusually difficult to implement in libraries. If
a library writes out its indexing logic using imperative code, the host
language compiler is not likely to be able to analyze it. 

\marginpar{This good stuff is buried!}
Our dispatch mechanism permits a novel solution. If a multiple dispatch system
supports variadic functions and argument ``splicing'' (the ability to pass a
structure of $n$ values as $n$ separate arguments to a function), then
indexing behavior can be defined as method signatures.

This solution is still a compromise among the factors outlined above, but it
is a new compromise that provides a net-better solution.

%TODO more

\subsection{\texttt{index\_shape}}

Below we define a function \texttt{index\_shape} that computes the shape of a
result array given a series of index arguments. We show three versions, each
implementing a different rule that users in different domains might want:

{\small
\begin{verbatim}
# drop dimensions indexed with scalars
index_shape() = ()
index_shape(i::Real, I...) = index_shape(I...)
index_shape(i, I...) = tuple(length(i), index_shape(I...)...)
\end{verbatim}
}

\noindent Notes: \newline
1. \verb+i::Real+ denotes a scalar which is dropped \newline
2. \verb+I...+ denotes the remainder of the arguments to be parsed next \newline
3. \verb+length+ counts  elements in an array index, thereby flattening \verb+i+
4. The rightmost three dots flatten the tuple


{\small
\begin{verbatim}
# drop trailing dimensions indexed with scalars
index_shape(i::Real...) = ()
index_shape(i, I...) = tuple(length(i), index_shape(I...)...)
\end{verbatim}
}

\noindent Notes: \newline
1. \verb+i::Real...+ denotes trailings scalars  which are dropped \newline


{\small
\begin{verbatim}
# rank summing (APL)
index_shape() = ()
index_shape(i, I...) = tuple(size(i)..., index_shape(I...)...)
\end{verbatim}
}


\noindent Notes: \newline
1. \verb+size(i)+ in contrast to \verb+length(i)+ incorporates component index shapes \newline

These definitions are concise, easy to write, and possible for a compiler to
understand fully using straightforward techniques.

%TODO: point out how this combines the ``object part'' and the ``array part''
%into a coherent whole.
%This is really a statement about implementing array semantics in language X
%using language X itself, and in particular using intrinsic features for handling
%objects to also handle arrays.

The result type is determined using only dataflow type inference, plus a rule
for splicing an immediate container (the type of \texttt{f((a,b)...)} is the
type of \texttt{f(a,b)}). Argument list destructuring takes place inside the
type intersection operator used to combine argument types with method
signatures.

This approach does not depend on any heuristics. Each call to
\texttt{index\_shape} simply requires one recursive invocation of type
inference. This process reaches the base case \texttt{()} for these
definitions, since each recursive call handles a shorter argument list (for
less-well-behaved definitions, we might end up invoking a widening operator
instead).

\begin{verbatim}

diverge() = randbool() ? () : tuple(1, diverge()...)

\end{verbatim}

This is an example of indexing behavior that is not amenable to useful static
analysis, since each branch of \code{diverge()} has different types.

%TODO say something about how types of tuples in Julia are defined.

Such code would throw a type error in languages requiring statically
checkableness such as Haskell. But in Julia, this is still allowed just that
the compiler may not have useful information from static analysis and so may
not run as fast. In Repa, the top priority is to appease the type system of
Haskell, with performance and user interface secondary. We think it should be
the other way round.

\section{Array views (Dahua's stuff)}

Also interesting are array views. In certain cases of subarray slicing, it is
possible to keep the data in place and return just a view (pointer) to the
data instead of creating a new copy in memory. In this case the same
infrastructure that applies to full arrays automatically works for subarrays.
Views are just another thing that implement the array protocol: (length, size,
getindex). Is there a point here about code reuse? Maybe but Jeff thinks it's
not crucial.

\section{Extensions of the idea}

Units is another thing like array rank. - It is a space of computations where
most of the time, Exponents on units can often be known statically but in
principle there are not, sometimes. But we give people the same expressive
ability in the same language to do everything anyway. Changes ``statically
analyzable + fast" versus ``not analyzable + slow" from a property of the
language to a property of problem.

\section{Conclusions}

A well-designed language force programmers to write code in ways that are
amenable to static analysis. Technically speaking you can say that the
inability to do static analysis is a weakness of compiler implementation, but
this is also a point about the ``power'' of a language. Compare Julia to the
past: language abstraction mechanisms were not powerful enough to define
$n$-arrays. The behaviors had to be built in to the compiler (or inside a very
elaborate C library like NumPy). In Julia, method signatures plus a general-
purpose (not array-specific) type inference algorithm can do it. The
combination turns out to be very important: if you only have multiple
dispatch, you just have a nifty-but-slow way to write these things. If you
just have type inference, then you are quite limited in what you can
realistically infer --- for example an explicit loop over the arguments to
getindex is too dynamic, and you can't really infer what it does.

Languages have to compromise between static (compiler) analysis - accept more
restrictions so that you can do more static analyses - vs user ease of writing
programs. Other dynamic languages lack extensive static analysis capabilities
in their default implementations. Implementations with some modicum of static
analysis can result in large performance gains, like FALCON or Hack
\cite{Verlaguet:2014hn}. But in Julia, we are going one step further and
turning the combination of multiple dispatch and automatic type inference from
a performance optimization hack, into a genuine feature of the programming
language, and designing a programming language to fully leverage the power of
this combination. Automatic type inference reduces the need for manual,
tedious type annotation and reduced redundancy in code. At the same time,
types in dispatch signatures give you access to behavior specialization through
multiple dispatch, thus enabling better performance. 

In contrast with statically typed languages like Haskell, Julia by virtue of
being a dynamic language can take a more liberal view on type correctness.
While Julia makes every attempt to infer type information from static
analysis, Julia does not create any issues over type correctness, but rather
execute code anyway in the absence of detailed type information. Contrast this
with the behavior of C++, which is a static language with separate static and
dynamic semantics. C++ resolves function overloads at compile time; this isn't
something you can do at runtime.

Automatic type inference allows for a lot of static analysis without a static
type system. Julia says we do a lot statically without a static type system.
The design idea is that languages with static types assert that the latter are
necessary for correctness. But it is also possible that with static analysis
that the code can be more performant due to specialization.

%\appendix
%\section{Appendix Title}
%
%This is the text of the appendix, if you need one.

\acks

The authors gratefully acknowledge the enthusiastic participation of the Julia
developer community for many stimulating discussions on the topic of array
implementations in the Julia language.

%TODO thank Intel, Wade, and perhaps NSF Solar here

% We recommend abbrvnat bibliography style.

\bibliography{refs}{}
\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

%\begin{thebibliography}{}
%\softraggedright

%\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
%P. Q. Smith, and X. Y. Jones. ...reference text...

%\end{thebibliography}

\end{document}
