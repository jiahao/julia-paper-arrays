\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:
% preprint      Remove this option only once the paper is in final form.

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}

\newcommand{\ALGOL}{A\textsc{LGOL}}
\newcommand{\MATLAB}{\textsc{MATLAB}}
\newcommand{\Mathematica}{\textit{Mathematica}}
\newcommand{\code}[1]{\texttt{#1}}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{ARRAY '14}{June 15, 2014, Edinburgh, UK}
\copyrightyear{2014} 
%\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
%\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{Array implementations in Julia}    % These are ignored unless
\preprintfooter{Array implementations in Julia} % 'preprint' option specified.

\title{Array implementations in Julia}
\subtitle{Implementing arrays in a way that is amenable to compiler analysis}

\authorinfo{Jeff Bezanson \and Jiahao Chen \and Keno Fischer \and Alan Edelman}
           {MIT Computer Science and Artificial Intelligence Laboratory}
           {\{ bezanson, jiahao, kfischer, edelman \}@csail.mit.edu}

%TODO Get Dahua to sign on as coauthor
%\authorinfo{Dahua Lin}
%           {Toyota Technological Institute at Chicago}
%           {dhlin@ttic.edu}

\maketitle

\begin{abstract}
This is the text of the abstract.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

\keywords
Julia, multiple dispatch

\section{Introduction}

\begin{quotation}
``Unfortunately, it is very difficult for a designer to select in advance all the abstractions which the users of his language might need. If a language is to be used at all, it is likely to be used to solve problems which its designer did not envision, and for which abstractions embedded in the language are not sufficient.'' - Ref. \cite{Liskov:1974pb}
\end{quotation}

One-dimensional arrays are a simple and essential data structure found in
most programming languages. The multi-dimensional arrays required in
scientific computing, however, are a different beast entirely. Allowing
any number of dimensions entails a significant increase in complexity. Why?
The essential reason is that core properties of the data structure no
longer fit in a constant amount of space. The space needed to store the
sizes of the dimensions (the array shape) is proportional to the number
of dimensions. This does not seem so bad, but becomes a large problem
due to three additional facts.

% TODO: break up using enumerate
First, code that operates on the dimension
sizes needs to be highly efficient. Typically the overhead of a loop is
unacceptable, and such code needs to be fully unrolled. Second, in some
code the number of dimensions is a \emph{dynamic} property --- it is
only known at run time. Third, programs may wish to treat arrays with
different numbers of dimensions very differently. A vector (1d) might
have rather different behaviors than a matrix (2d) (for example, to
compute a norm). This kind of
behavior makes the number of dimensions a crucial part of program
semantics, preventing it from remaining a compiler implementation detail.

%TODO: Separate out point that dimensionality is semantically meaningful.

% TODO: break up using enumerate
These facts pull in different directions. The first fact asks for static
analysis. The second fact asks for run-time flexibility. The third fact asks
for dimensionality to be part of the type system, but partly determined
at run time (for example, via virtual method dispatch). Current approaches
choose a compromise. In some systems, the number of dimensions has a strict
limit (e.g. 3 or 4), so that separate classes for each case may be written
out in full. Other systems choose flexibility, and just accept that most
or all operations will be dynamically dispatched. Other systems might
provide flexibility only at compile time, for example a template library
where the number of dimensions must be statically known.

Arguably, arrays were the earliest data structure to be conceived of, and implemented on, computers. Implementation specifications for three-dimensional arrays go back to Konrad Zuse's Plankalk\"ul, and were also described in the reference implementation of FORTRAN I on the IBM 704 \cite{Backus:1956pr,Backus:1957fa}. The programmers manual describes that:

%TODO cite http://www.catb.org/retro/plankalkuel/

\begin{enumerate}
\item The elements of the array are stored sequentially in memory.
\item The elements are in column-major order.
\item The elements are stored from bottom right to top left.
\end{enumerate}

From this we can see that the first array language specification to be actually implemented specified that arrays were to be stored in a linear contiguous block in memory, that it was strided backward in memory and in column major style, and it had associated metadata about its shape which was partly user-controllable: the maximal rank was 3 (hard coded), but the size in each dimension could be user-specified.

Arrays are useful mostly because the element indices can be computed at run time.

%Footnote: The maximal rank of an array was increased to 7 in Fortran 77, and later to 15 in Fortran 2008.

These design decisions were hard-baked into the FORTRAN I compiler, and have guided the design of compilers ever since.

The arrays in Fortran were static arrays, where the size of the array is known and fixed at compile time and cannot be changed at runtime. (Dynamic arrays were introduced in the Fortran 90 standard with the \code{ALLOCATABLE} keyword.)

%CITE Compilers textbook

In contrast, the contemporaneous \ALGOL{} 60 programming language supported dynamic multidimensional arrays: not only were the arrays of arbitrary rank, but the size of the array could also change at runtime. This posed a challenge for compiler writers of the time. They introduced what was called the \textit{dope vector} \cite{Sattley:1960as, Sattley:1961as}, which contained the size and lower bound of each dimension, and the closely related \textit{storage mapping function} \cite[pp.~80--87]{Randell:1964a6} for the special case of 0-based indices and is essentially equivalent to \code{size(A::Array)} in Julia; both contain information needed to convert from the multidimensional array index to a linear memory offset.

%TODO Fortran started with only arrays; objects were added later: Arrays were the only compound type supported by Fortran. In Fortran however, arrays were a fundamental part of the language from the very beginning, whereas objects were not introduced until the advent of Object-Oriented Fortran and Fortran 2003.

%TODO Fortran dynamic arrays were only introduced officially with the ALLOCATABLE keyword of Fortran 90.

In Fortran however, arrays were a fundamental part of the language from the very beginning, whereas objects were not introduced until the advent of Object-Oriented Fortran and Fortran 2003.

In \Mathematica, the central data type is an expression \cite{mathematica:expr},
\url{}, whose parts can be referenced by the \code{[[]]}, or \code{Part}, symbol \cite{mathematica:part}: \code{A[[0]]} returns the head of the expression, and \code{A[[i]]} returns the $i^{th}$ argument. At the discretion of the internal compiler, \Mathematica{} arrays may be represented internally as expression trees, with each level of the tree represented as a continguous array of pointers first to the head and then to each part, or as ``packed arrays of machine-sized integers or real numbers'', a description which resembles the traditional array data structure \cite{mathematica:int}. To the user, however, arrays are simply nested lists \cite{mathematica:nl}, and the \code{[[]]} symbol naturally lends itself to the role of array indexing. This has two non-obvious consequences. First, it is possible to ``index'' an array with more indices than its rank; the extra indices would simply traverse into the expression contained within the matrix element. For example, \code{Array[f, {3, 4}][[3, 2]]} returns \code{f[3,2]}, whilst \code{Array[f, {3, 4}][[3, 2, 1]]} returns \code{3}. Thus the semantics of the \code{Part} symbol is more general than pure array indexing and allows for violations of the array data abstraction, which may come as a surprise to users. Second, \Mathematica{} interprets nested lists of depth $n$ as rank-$n$ multidimensionary arrays (which are termed ``tensors'' in \Mathematica), but at the same time implicitly assumes the isomorphism between, say, rank-4 arrays and matrices of matrices, which \Mathematica{} takes advantage of when displaying results in \code{MatrixForm}. For example, the code 
\begin{verbatim}
Zero[n_] := Table[0, ##] & @@ Table[{2}, {n}]
\end{verbatim}
generates the zero array of rank $n$ and size (2,2,\dots,2), which when displayed in \code{MatrixForm} will render as nested matrices and vectors, as demonstrated in Figure~\ref{fig:mathematica}.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{fig-mathematica/fig.pdf}
  \caption
   {\label{fig:mathematica} \Mathematica's \code{MatrixForm} displays of \code{Zero[n]}, the zero array of rank $n$ and size (2,2,\dots,2), for $n=3$ and $n=6$.}
\end{figure}

%% TODO: examples of systems limited to n==3

Whatever decision is made, rules must be defined for how various operators
act on dimensions. For now we will focus on indexing, since selecting
parts of arrays has particularly rich behavior with respect to
dimensionality. For example, if a single row or column of a matrix is
selected, does the result have one or two dimensions? Array implementations
prefer to invoke general rules to answer such questions. Such a rule might
say ``dimensions indexed with scalars are dropped'', or ``trailing
dimensions of size one are dropped'', or ``the rank of the result
is the sum of the ranks of the indexes'' (as in APL).

\subsection{Arrays in today's languages (literature review stuff)}

What languages are ``famous'' for arrays and how to they implement
arrays: APL, ZPL, Python/NumPy, C/Fortran. Implementation details:
row vs column major, indexing rules, etc. Implementation details constrain
how users have to deal with stuff.

Problems for static compilers for dynamics languages? Matlab in particular?

Problems with dynamic language implementations? Python?

Arrays suffer from dearth of abstraction - lots of manual implementation,
hand coded into the compiler, implementation is imposed on users.
Oftentimes the object system is disjoint from the arrays, e.g. R types,
because these subsystems were tacked onto the language at different
points in time. In NumPy, mechanisms of objects are not well used.
Detailed manual mechanisms in C code. Custom dispatch, generated code,
pretty crazy. NumPy arrays are Python objects but the mechanism of
the object system don't play a huge role in defining behavior. Objects
are lookup tables of behaviors, primarily centered around indexing
behavior. Single indexing lookup of symbols which are just integers.
Not very powerful - people think it is because it is dynamic dispatch.
We can generalize it to be tremendously more powerful.


\section{Body}

What does Julia do?


Use the object system to implement Arrays. A coherent unification
of things that were typically disjoint, e.g. in R. 

Array is a pair of (shape, linear data storage) = metadata of bounded
size O(1) + raw data O(N).

Most people do this too, maybe at most add a stride. With this basic
structure we can define behavior for multidimensional arrays. Can have
lots of nontrivial design problems.

Here we consider the indexing rules. How to compute shapes of subarrays.
How to deal with singleton dimensions is but a very special case even
though superficially the rules mention them explicitly.

Here is a good place to use Julia code to codify all the various indexing
rules that various languages have

In Julia, such indexing rules are defined in exactly one place and
can be changed later if so desired.

\subsection{The need for flexibility in array indexing rules}

Our goal here is a bit unusual: we are not concerned with which rules
might work best, but merely with how they can be specified, so that
domain experts can experiment.

In fact different domains want different things. E.g. in images, each
dimension might be quite different, e.g. time vs. space vs. color,
so you don't want to drop or rearrange dimensions very often.

In practice we may all have to reach a consensus on what rules to
use, but it should not be enforced upon uses by pure technical implementation
convenience. The point is that in Julia, these are not enforced a
priori. Sometimes the distinctions between the various indexing rules
are semantically meaningful and that's when this flexibility becomes
particularly valuable. For example Tim Holy's image 4-arrays. Quantum
mechanics when you average out multiple indistinguishable particles.
$n$-point correlations functions where which $k$ indices you average out defines
any number of lower-point $n-k$ point correlation functions.

Other semantic issues: embeddings of vectors as matrix columns? Matrix version
of circle-ellipse problem \cite{Halbert:1987ut}, a canonical example of the violation of the Liskov substitution principle \cite{Liskov:1987da}, analogous to real-complex embedding. Transposition?

\subsection{\MATLAB's indexing rules}

Arrays are the most fundamental data type in \MATLAB{}, and all \MATLAB{} arrays must have at least two dimensions. Consequently, row vectors, column vectors and scalars are treated as isomorphic to matrices with dimensions $1\times N$, $N\times1$ and $1\times1$ respectively. This reflects \MATLAB's original design principle as a ``\textbf{matrix} laboratory".

Not having pure scalars can be mathematically troublesome. For example, the product $A_{m\times n} \times B_{1\times 1} \times C_{n\times p}$ is invalid under the ordinary rules of matrix multiplication for $n\ne1$; however, \MATLAB{} would allow the evaluation of this product by interpreting $B$ as a scalar which commutes with $A$ and $C$, allowing this product to be evaluated as the product of the scalar $B_{11}$ and the matrix product $A_{m\times n} \times C_{n\times p}$.

%TODO Find examples where this kind of interpretation breaks commutativity or associativity. Matrices of matrices or matrices of quaternions?

Also, all arrays are treated as if they have an infinite number of trailing singleton dimensions. This reflects a later design decision in \MATLAB{} to support multilinear algebra \cite{matlabman:ma},  albeit in a manner that does not quite mesh with the world of ordinary linear algebra. This unfortunately clashes with the \textit{linear indexing} rule in \MATLAB{} that indexing a multidimensional array with a single index automatically triggers a reshape. For example, for $A_{3\times4}$, there is an ambiguity as to whether \texttt{A[5]} means \texttt{B=A[:];B[5]} or \texttt{A[5,1,1,\dots]}, and this ambiguity has to be resolved with an arbitrary design decision.

\MATLAB{} also allows indexing with matrices, but they are treated as if they were flattened for indexing purposes, i.e.

\texttt{A(M1, M2, \dots) = A(M1(:), M2(:)), \dots}

There is also something very funky with \MATLAB's cell arrays. You have to index into them using \texttt{A\{...\}} instead of the usual \texttt{A(...)}.

%TODO Find an example for which this causes problems?

\subsection{Ground rules}
Here are our ground rules:

\begin{enumerate}
\item You can't manually implement the behavior inside the compiler
\item The compiler must be able to reasonably understand the program
\item The code must be reasonably easy to write
\end{enumerate}


How are such rules implemented? For a
language with built-in multidimensional arrays, the compiler will
analyze indexing expressions and determine an answer using hard-coded
logic.

% TODO: grab example from one of these compilers
However, this approach is not satisfying: we would rather
implement the behavior in libraries, so that different kinds of arrays
may be defined, or so that rules of similar complexity may be
defined for other kinds of objects. But these kinds of rules are
unusually difficult to implement in libraries. If a library writes out
its indexing logic using imperative code, the host language compiler
is not likely to be able to analyze it. Using compile-time abstracton
(templates) would provide better performance, but such libraries tend
to be difficult to write (and read), and the full complement of
indexing behavior expected by technical users strains the capabilities
of such systems.

% TODO: insert actual example of numpy

Our dispatch mechanism permits a novel solution. If a multiple dispatch
system supports variadic functions and argument ``splicing'' (the ability
to pass a structure of $n$ values as $n$ separate arguments to a function),
then indexing behavior can be defined as method signatures.

This solution is still a compromise among the factors outlined above,
but it is a new compromise that provides a net-better solution.
% TODO more

\section{\texttt{index\_shape}}

Below we define a function \texttt{index\_shape} that computes the
shape of a result array given a series of index arguments. We show
three versions, each implementing a different rule that users in
different domains might want:

% TODO: point out array = (shape, data), so those are the two parts
% we need to handle. In julia the ``data'' part is not a first class
% object; it is not directly exposed to the user, but this is more
% of an implementation detail.

\begin{verbatim}
# drop dimensions indexed with scalars
index_shape() = ()
index_shape(i::Real, I...) = index_shape(I...)
index_shape(i, I...) = tuple(length(i), index_shape(I...)...)
\end{verbatim}

\begin{verbatim}
# drop trailing dimensions indexed with scalars
index_shape(i::Real...) = ()
index_shape(i, I...) = tuple(length(i), index_shape(I...)...)
\end{verbatim}

\begin{verbatim}
# rank summing (APL)
index_shape() = ()
index_shape(i, I...) = tuple(size(i)..., index_shape(I...)...)
\end{verbatim}

Inferring the length of the result of \texttt{index\_shape} is sufficient
to infer the rank of the result array.

These definitions are concise, easy to write, and possible for a
compiler to understand fully using straightforward techniques.

% TODO: point out how this combines the ``object part'' and the
% ``array part'' into a coherent whole.

%TODO put in some cleaned-up version of Jeff's derivation of the first case.

The result type is determined using only dataflow type inference, plus a
rule for splicing an immediate container (the type of \texttt{f((a,b)...)} is
the type of \texttt{f(a,b)}). Argument list destructuring takes place inside
the type intersection operator used to combine argument types with method
signatures.

This approach does not depend on any heuristics. Each call to \texttt{index\_shape}
simply requires one recursive invocation of type inference. This process reaches
the base case \texttt{()} for these definitions, since each recursive call
handles a shorter argument list (for less-well-behaved definitions, we might
end up invoking a widening operator instead).


\begin{verbatim}
diverge() = randbool() ? () : tuple(1, diverge()...)
\end{verbatim}

\section{Array views (Dahua's stuff)}

Also interesting are array views. In certain cases of subarray slicing,
it is possible to keep the data in place and return just a view (pointer)
to the data instead of creating a new copy in memory. In this case
the same infrastructure that applies to full arrays automatically
works for subarrays. Views are just another thing that implement the
array protocol: (length, size, getindex). Is there a point here about
code reuse? Maybe but Jeff thinks it's not crucial.


\section{Extensions of the idea}

If we redefine Array to be (shape, stride pattern and linear data
store), this would be sufficient to extend to row major vs column
major. We would have a language that is majorization-agnostic, which
defies the traditional classification.

Data locality is another interesting issue. This can be part of the definition
of Arrays. Data locality is an open problem. Maybe we need multiple linear data stores. Maybe we need complicated locality maps to be part of the associated metadata. But the point is that with a flexible definition in the Base library, not hard-coded into the compiler, these are design decisions that can be revisted and modified if necessary.

Locality and majorization order are two facets of the more general issue - where are the data located? With the right abstractions you can implement and experiment with them, change them without too much difficulty. We may not have all the answers but you can experiment with the possibilities in the same spelling.

Another interesting possibility is the treatment of symmetric and antisymmetric
tensors, since the ability to define custom striding rules would allow for
representations of such quantities that eliminate the redundant storage of
symmetry-equivalent components. (This becomes very important in higher
dimensions $d$ since multiplicity of redundant storage grows as O($d!$).

%\appendix
%\section{Appendix Title}
%
%This is the text of the appendix, if you need one.

\acks

The authors gratefully acknowledge the enthusiatic participation of the Julia developer community for many stimulating discussions on the topic of array implementations in the Julia language.

% We recommend abbrvnat bibliography style.

\bibliography{refs}{}
\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

%\begin{thebibliography}{}
%\softraggedright

%\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
%P. Q. Smith, and X. Y. Jones. ...reference text...

%\end{thebibliography}

\end{document}
